21/08/01 06:47:57 INFO org.sparkproject.jetty.util.log: Logging initialized @3743ms to org.sparkproject.jetty.util.log.Slf4jLog
21/08/01 06:47:57 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_292-b10
21/08/01 06:47:57 INFO org.sparkproject.jetty.server.Server: Started @3875ms
21/08/01 06:47:58 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@67a69983{HTTP/1.1, (http/1.1)}{0.0.0.0:44685}
21/08/01 06:47:59 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
21/08/01 06:48:07 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
21/08/01 06:48:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
21/08/01 06:48:10 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
21/08/01 06:48:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-testing-pyspark/.spark-bigquery-local-1627800478219-ec5e1991-d6e8-4d13-aafc-8356850029d6/_temporary/0/_temporary/attempt_202108010648096007144894293105705_0002_m_000000_2/' directory.
21/08/01 06:48:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-testing-pyspark/.spark-bigquery-local-1627800478219-ec5e1991-d6e8-4d13-aafc-8356850029d6/_temporary/0/_temporary/' directory.
21/08/01 06:48:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-testing-pyspark/.spark-bigquery-local-1627800478219-ec5e1991-d6e8-4d13-aafc-8356850029d6/' directory.
21/08/01 06:48:13 INFO com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=GermanCredit, projectId=silver-argon-320416, tableId=German_Credit_final}}, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://dataproc-testing-pyspark/.spark-bigquery-local-1627800478219-ec5e1991-d6e8-4d13-aafc-8356850029d6/part-00000-54d2ffcd-803d-468a-aa37-7f01fb76bec8-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null}. jobId: JobId{project=silver-argon-320416, job=884a7665-33e3-466e-8518-6080591b9d4b, location=US}
21/08/01 06:48:15 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=GermanCredit, projectId=silver-argon-320416, tableId=German_Credit_final}}. jobId: JobId{project=silver-argon-320416, job=884a7665-33e3-466e-8518-6080591b9d4b, location=US}
21/08/01 06:48:15 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to silver-argon-320416.GermanCredit.German_Credit_final. jobId: JobId{project=silver-argon-320416, job=884a7665-33e3-466e-8518-6080591b9d4b, location=US}
21/08/01 06:48:16 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@67a69983{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
